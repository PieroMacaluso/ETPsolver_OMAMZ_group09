\documentclass[11pt, a4paper, leqno]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{parskip}
\setlength{\parskip}{0.3\baselineskip}%
\usepackage{libertine} % font bello
\usepackage[paper=a4paper,top=0.5in,bottom=0.8in,right=0.6in,left=0.6in]{geometry} % margini

\usepackage{enumitem} % liste piÃ¹ compatte (se ce ne saranno)
%\setlist[enumerate]{itemsep=0.0em}
\setlist[itemize]{itemsep=0.0em}
\usepackage[compact]{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{titlesec}
\usepackage[]{algorithm2e}

%\titlespacing*{\section}{0pt}{0.5\baselineskip}{0.05\baselineskip}
%\titlespacing*{\subsection}{0pt}{0.5\baselineskip}{0.05\baselineskip}

\newcommand{\nome}[2]{
	\begin{minipage}[t]{0.185\linewidth}
		\centering #1\par
		\centering\small (#2)\par
	\end{minipage}
}

% Questo era utile per evidenziare le cose "dubbie e da correggere"
% Poi si elimina il comando cosi' non restano \boh{} in giro
%\usepackage{xcolor}
%\newcommand{\boh}[1]{\textcolor{red}{#1}}

\newcommand{\eqp}[1]{\textit{(eq. \ref{#1})}}
\newcommand{\eq}[1]{\textit{\ref{#1}}}

\begin{document}	
	\begin{center}
		{\huge\textbf{OMA Final Report}}\par
		\vspace{0.3em}
		{\large\textbf{Group 9}}\par
		\vspace{1em}
		\nome{Piero Macaluso}{s252894}
		\nome{Lorenzo Manicone}{s217189}
		\nome{Donato Tortoriello}{s205639}
		\nome{Ludovico Pavesi}{s246422}
		\nome{Alberto Romano}{s254036} % 164371
	\end{center}
	
	\section{Initial solution generation}
	
	\subsection{Initial attempts}
	
	\paragraph{Random}
	
	Initially we tried generating random starting solutions: that is, to each exam assign a random time slot. This was very fast, but even after thousands of runs we couldn't find even a single feasible solution on ``real'' instances. It worked on the ``test'' instance, however.
	
	\paragraph{Greedy 1.0 (``Extremely Stupid But Also Cultured Greedy'')}
	
	We tried to implement a simplistic greedy algorithm that iterates over each exam, picks an available (non conflicting) timeslot at random, and schedules the exam there.\\
	This never yielded a feasible solution for any of the real instances, as when it got near the end of the exam list there were no more non-conflicting time slots available.
	
	\paragraph{Greedy 2.0 (``Timeslots and Conflicts'')}
	
	We tried sorting exams by some more meaningful criteria than their ID, i.e. by number of conflicting exams, and then assigning a random available timeslot to each exam in that order.\\
	This solution was simple but found feasible solutions only for \textit{instance01}, so we decided to sort them first by the number of unavailable timeslots (timeslots with conflicting exams already scheduled), then by the number of conflicts. Since the order depends on current solution (each time an exam is scheduled, the ``number of unavailable timeslots'' for some other exams change), the list needed to be sorted again at each step.\\
	%\begin{algorithm}[H]
	%		%		%\KwData{dati}
	%		%		%\KwResult{risultato}
	%		%		%initialization\;
	%		$list \gets \text{all exams to be scheduled, sorted by }\textit{(number of unavailable slot, number of conflicts)}$\;
	%		\While{$list\text{ is not empty}$}{
	%			$E \gets \textit{ first element from }list$\;
	%			$T \gets \textit{ random available timeslot}$\;
	%			schedule $E$ in $T$\;
	%			remove $E$ from $list$\;
	%			sort $list$ again\;
	%		}	
	%		\caption{Algorithm with Timeslots and Conflicts sorting}
	%	\end{algorithm}
	But even so, there were still no feasible solutions for the other instances.
	
	\subsection{Greedy 3.0 (``Timeslots, Conflicts and Unscheduling'')}
	
	With the previous algorithm, the tipping point was the absence of available timeslots for the current exam when getting near the end of the list.
	
	So we decided to unschedule all the conflicting exams of the current one when that situation was encountered, sort and retry scheduling them again.
	
	\begin{algorithm}[H]
		%		$backup \gets \text{current solution (even an empty one)}$\;
		%		$sol \gets \text{current solution (even an empty one)}$\;
		$retries \gets 0$\;
		$list \gets \text{exams to be scheduled sorted by }\textit{number of unavailable slot, number of conflicting exams}$\;
		\While{$list$ is not empty}{
			\If{\textit{retries} > limit}{
				\Return{no feasible solution found}\;
				%				$sol \gets backup$\;
				%				restart the alogithm\;
			}
			$E \gets \textit{ first element from }list$\;
			$T \gets \textit{ random available timeslot}$\;
			\eIf{$T$ is valid (not null)}{
				schedule $E$ in $T$\;
				remove $E$ from $list$\;
			}{
				\ForEach{conflicting exam $C$ of $E$}{
					unschedule $C$\;
				}
				$retries \gets retries + 1$\;
			}
			sort $list$ again\;
		}
		\Return{feasible solution found}\;
		\caption{Timeslots, Conflicts and Unscheduling}
	\end{algorithm}
	
	In order to avoid unstable or ``oscillating'' situations, we fixed a maximum number of ``retries'': if this limit is exceeded, the algorithm returns anyway leaving an infeasible solution, which will be discarded and another attempt will be made, from a different initial solution or a different neighbor (see \ref{neighbors}). This was a great step forward, as it worked with all instances. There was only one problem: in more complex instance it takes more time to get a feasible solution.
	
	We also tried sorting by ``number of unavailable slot, then number of conflicting students'', but that seemed to yield more infeasible solutions and the feasible ones weren't any better than before.
	
	Retries limit has been set to $\frac{1}{3}$ of total exams, which gave acceptable results from empirical testing.
	
	This algorithm was a great step forward, working with all instances. There was only one problem: in larger instances, like \textit{instance06}, it took a lot of time to generate an initial solution. With the use a profiler, we determined that the slowdown was due to the ``number of unavailable timeslots'' procedure that required, for each exam, to iterate over its conflicting exams, checking if and were are they scheduled.
	
	It needed to compute $n$ values, but being part of a comparison function, it was run approximately $2\cdot n \log(n)$ times ($n \log(n)$ comparisons between 2 exams), $n$ being length of $list$. We implemented a ``caching layer'' based on an \textsc{HashMap}, reset each time before sorting, and that greatly improved performance
	
	\section{Further solutions generation}
	
	\subsection{Genetic algorithm}
	We tried running a genetic algorithm implementation, with ``Random'' and ``Greedy 1.0'' initial solutions especially. It worked fine on the test instance, however on real instances it never produced a feasible solution, not even after thousands of iterations.
	
	\subsection{Simulated Annealing}
	
	We decided to use a simulated annealing algorithm, implemented in its most standard form, as a starting point.
	
	\paragraph{Temperature, probability and delta}
	
	Probability standard formula used: $exp\left(\frac{\mathrm{currentCost}-\mathrm{neighborCost}}{\mathrm{temperature}}\right)$.
	
	We decided to start with a probability of $50\%$, we inverted that formula and used initial solution cost as $\mathrm{neighborCost}$ and the cost of that same solution optimized with local search (see \ref{local}) as $\mathrm{currentCost}$ to obtain the initial temperature. Temperature then decreases with time.
	
	\paragraph{Neighbor generation}
	\label{neighbors}
	
	Neighbor solutions are generated by taking the current (feasible) solution, unscheduling a percentage of all the exams and rescheduling them with ``Greedy 3.0'' algorithm.
	
	After some experiments, we found that unscheduling between $20\%$ and $30\%$ of exams gave the best and most consistent results, unscheduling around $10\%$ sometimes gave exceptional solutions while other times some bad ones, while unscheduling more than $50\%$ of exams was too slow on larger instances.
	
	Ultimately we settled on starting from $30\%$ and decreasing the percentage as time goes on according to temperature. We also placed a lower limit of $10\%$: if calculated percentage is less than $10\%$, it is set to $10\%$. This makes the decrease nonlinear but doesn't really seem to affect the generated solutions in a bad way and it was easy to code.
	
	Since the Greedy algorithm may fail, when that happens neighbor generation is retried with a different set of unscheduled exams. This happens very rarely in all instances but often in \textit{instance02}.
	
	\subsection{Timetable swapping}
	
	We enhanced simulated annealing by adding a timetable swapping algorithm, which runs on the generated neighbor. It simply tries to swap all exams for each couple of time slots, and does it if the solution improves. It's relatively slow but solutions improve much faster.
	
	\subsection{Local search}
	\label{local}
	
	We further enhanced simulated annealing by adding a local search on each generated solution (initial or neighbor). Our local search works like this:
	
	\begin{algorithm}[H]
		%		$sol \gets \text{current solution (even an empty one)}$\;
		$list \gets \text{all exams to be scheduled, sorted by }\textit{number of conflicts}$\;
		\ForEach{exam $E$ in $list$}{
			$T \gets \text{timeslot where }E\text{ is scheduled}$\;
			$C \gets \text{cost of exam }E\text{ in }T\text{ time slot according to objective function}$\;
			\ForEach{timeslot $T'\neq T$}{
				$C' \gets \text{cost of exam }E\text{ in }T'\text{ time slot according to objective function}$\;
				\If{$C'<C$}{
					schedule $E$ in $T'$\;
					$C \gets C'$\;
				}
			}
		}
		\caption{Local search}
	\end{algorithm}
	
	Since each time an exam is rescheduled, the cost (penalty) of any other exam may change but the algorithm runs over exams only once, it may provide better results if run multiple times. We exploited this fact by placing a minimum improvement percentage: if after each run the solution has improved by more than the minimum percentage, local search is run again. The higher the required minimum improvement, the less times local search will be run.
	
	After some testing, we found that $10\%$ was a good starting point for initial solutions, as in most cases ran local search once or twice and yielded a solution better by around $5\%$ (\textit{instance01}, runs once) to $20\%$ (\textit{instance02} and \textit{instance06}, runs twice most of the times), which helped simulated annealing to get an even better solution sooner.
	
	We decided to run local search on generated neighbors too, as often they were far worse than an optimized initial solution and so were never picked as ``better'' by simulated annealing. There, we start from $10\%$ and decreased the minimum improvement percentage linearly as time goes on, until almost $0\%$: in our testing we found that, even with percentages as low as $0.001\%$, local search terminates in at most 5 iterations, so there was no risk of letting the algorithm run forever.
	
	Our local search has linear time complexity in the number of exams, since time slots are always far less than exams, so it's very fast. Sorting exams isn't really done by this algorithm, as the values are computed when reading initial data and cannot change as the program runs.
	
	Doing timetable swaps before local search yields better average solutions in \textit{instance06} while it has no measurable effect on other instances, so we left it in that order.
	
	\section{Other considerations}
	
	\paragraph{Multithreading}
	
	Our program exploits the power of multiple cores, by running multiple independent threads. Each thread generates an initial solution and optimizes it as described until time runs out. Each time it improves it thread-local best solution, it compares that to the best solution that other threads have found, and save it as the ``global'' best if it's better.
	
	\paragraph{Profiling}
	
	We used a profiler multiple times to find performance bottlenecks in code. This for example suggested us the idea to add caching in the Greedy algorithm (which reduced its running time from 360 ms to 120 ms, a considerable improvement since it runs hundreds of times) and using \textsc{HashMap}s with low load factors instead of \textsc{TreeMap}s.
	
	\paragraph{Running for longer than 30 seconds}
	
	From limited testing, we found that the program yields increasingly better solutions for 2-3 minutes, then usually they stop improving. We haven't tried running  it for more than 5 minutes.
	
\end{document}
